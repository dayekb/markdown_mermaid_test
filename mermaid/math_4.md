``` mermaid
graph TD
    A["1670s: Исчисление<br>(Ньютон, Лейбниц)"] --> B[Производные]
    B --> C["Частные производные<br>(Эйлер, Лагранж, 18 век)"]
    C --> D["Градиент<br>(Гамильтон, 19 век)"]
    D --> E["Градиентный спуск<br>(Коши, 1847)"]

    F["1805: МНК<br>(Гаусс, Лежандр)"] --> G[Линейная регрессия]
    G --> H[Функция потерь — L2]
    H --> I[Оптимизация моделей]

    E --> I
    I --> J["Стохастический градиентный спуск<br>(Роббинс и Монро, 1951)"]

    K["1713–1763: Теория вероятностей<br>(Бернулли, Байес, Лаплас)"] --> L[Байесовский вывод]
    L --> M[Логистическая регрессия, 1940s–50s]

    N["1840: Сигмоида<br>(Ферхюльст)"] --> M
    M --> O["Персептрон<br>(Розенблатт, 1957)"]
    O --> P[Нейронные сети]

    Q["1948: Теория информации<br>(Шеннон)"] --> R[Кросс-энтропия]
    R --> S[Функция потерь для классификации]
    S --> M
    S --> P

    T["1960s: Backpropagation (идея)<br>(Брайсон, Хо)"] --> U["1986: Обратное распространение<br>(Румельхарт, Хинтон, Уильямс)"]
    U --> V[Обучение глубоких сетей]

    W["1989: Теорема об универсальной аппроксимации<br>(Сиборг, Хорник)"] --> V
    V --> X["LeNet-5 (ЛеКун, 1998)"]

    Y[2012: AlexNet] --> Z[Глубокое обучение — прорыв]
    X --> Y
    J --> Y

    AA["2012: Dropout<br>(Хинтон)"] --> AB[Регуляризация нейросетей]
    AB --> Z

    AC["2015: Автодифференцирование<br>(TensorFlow, PyTorch)"] --> AD[Автоматический backprop]
    AD --> Z

    AE["2017: Трансформеры<br>(Васвани и др., Attention is All You Need)"] --> AF[Современные LLM]
    Z --> AF

   %% style Z fill:#e0f7fa,stroke:#006064
    %%style AF fill:#b2dfdb,stroke:#006064

```